# wget notes

|||wget download entire webpages websites

```sh
    wget \
    --quiet \
    --page-requisites \
    --span-hosts \
    --convert-links \
    --adjust-extension \
    --no-parent \
    --execute robots=off \
    --random-wait \
    --user-agent=Mozilla \
    --continue \
    --no-clobber \
    --directory-prefix="$TARGET_DOWNLOAD_DIR_NAME" \
    --domains ${DOMAIN_STRIPPED} \
    ${URL}
```

```bash
wget \
 --page-requisites \
 --span-hosts \
 --convert-links \
 --adjust-extension \
 --no-parent \
 --execute robots=off \
 --random-wait \
 --user-agent=Mozilla \
 --continue \
 --no-clobber \
 --directory-prefix="folder/subfolder example.com path" \
 --domains example.com \
 https://www.example.com/path/

```

```bash
wget \
 --page-requisites \
 --span-hosts \
 --convert-links \
 --adjust-extension \
 --no-parent \
 --execute robots=off \
 --wait=30 \
 --random-wait \
 --user-agent=Mozilla \
 --continue \
 --no-clobber \
 https://www.example.com/path/
```

```bash
wget \
 --page-requisites \
 --span-hosts \
 --convert-links \
 --adjust-extension \
 --no-parent \
 --execute robots=off \
 --wait=2 \
 --random-wait \
 --user-agent=Mozilla \
 --continue \
 --no-clobber \
 --directory-prefix=folder/subfolder example.com path \
 --domains ${WEBSITE-DOMAIN} \
  https://www.example.com/path/
```

```bash
wget \
 --mirror \
 --page-requisites \
 --span-hosts \
 --convert-links \
 --adjust-extension \
 --no-parent \
 --execute robots=off \
 --wait=1 \
 --random-wait \
 --user-agent=Mozilla \
 --continue \
 --no-clobber \
 --timestamping \
 --directory-prefix=~/Downloads/untitled\ folder\ 3/ \
 --domains example.com \
  https://www.example.com/path/
```

```bash
wget \
 --mirror \
 --recursive \
 --page-requisites \
 --span-hosts \
 --convert-links \
 --adjust-extension \
 --no-parent \
 --execute robots=off \
 --wait=1 \
 --random-wait \
 --user-agent=Mozilla \
 --continue \
 --no-clobber \
 --timestamping \
 --directory-prefix=~/Downloads/untitled\ folder\ 4/ \
 --domains example.com \
  https://www.example.com/path/
```

> References
>
> <https://superuser.com/questions/14403/how-can-i-download-an-entire-website>
>
> <https://www.linuxjournal.com/content/downloading-entire-web-site-wget>
>
> <https://www.gnu.org/software/wget/manual/wget.html#Spanning-Hosts>
>
> <https://linux.die.net/man/1/wget>
>
> <https://stackoverflow.com/questions/1078524/how-to-specify-the-location-with-wget>

---

Save an entire webpage, with all images and css, into just one folder and one file, with wget?

> References
> <https://superuser.com/questions/1150495/save-an-entire-webpage-with-all-images-and-css-into-just-one-folder-and-one-fi>

You/WGET can't. It can download all linked resources in one given download, but that would result in multiple folders due to its nature of crawling, not interpreting (and not being bound to HTTP either).

Also your impression is too narrow: there are web browsers which can save pages into MHT files/archives, which is even a standard - see https://en.wikipedia.org/wiki/MHTML

---

How to get WGET to download exact same web page html as browser

> References
> <https://askubuntu.com/questions/411540/how-to-get-wget-to-download-exact-same-web-page-html-as-browser>

As [roadmr][2] noted, the table on this page is generated by javascript. wget doesn't support javascript, it just dumps the page as received from the server (ie before any javascript code runs) and so the table is missing.

You need a headless browser that supports javascript like [phantomjs][1]:

    $ phantomjs save_page.js http://example.com > page.html

with save_page.js: 

    var system = require('system');
    var page = require('webpage').create();

    page.open(system.args[1], function()
    {
        console.log(page.content);
        phantom.exit();
    });

Then if you just want to extract some text, easiest might be to render the page with w3m:

    $ w3m -dump page.html

and/or modify the phantomjs script to just dump what you're interested in.

  [1]: http://phantomjs.org
  [2]: https://askubuntu.com/users/30589/roadmr

===

This also don't work, for example cotrino.com/lifespan – mrgloom Jan 12 '18 at 9:19

JS generated links wont work with that – QkiZ Jul 28 '18 at 19:21

2018: PhantomJS project is suspended until further notice :( – 1rq3fea324wre Sep 13 '18 at 1:44

This solution is only for downloading pages from specified urls. How do you pipe wget's site crawling mechanism with it? Also, what would the script look like with headless chrome? – Phil Mar 6 '19 at 9:15

It's not working. Error is TypeError: Attempting to change the setter of an unconfigurable property. TypeError: Attempting to change the setter of an unconfigurable property. – Prvt_Yadav Jul 13 at 9:06

---
